# -*- coding: utf-8 -*-
"""bp_model_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQxivdjuOjIaTQ0FDCMKk--3ztsOzePQ

# **Import Libraries**
"""

# Structure it into (1)data preprocessing/preparation, (2)model training, (3)evaluation metrics

# EDA
import numpy as np
import pandas as pd
import seaborn as sns
import missingno as msno
import matplotlib.pyplot as plt


# DP/P
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV

# AI
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier


# EM
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import joblib

import warnings
warnings.filterwarnings("ignore")

"""# **Load and Preprocess Data**"""

# Load the dataset with the updated heart rate column
data = pd.read_csv(r"/content/Hypertension-risk-model-main.csv") # Path to your dataset
data.head()

data.describe()

data.rename(columns={'Risk':'TenYearCHD'}, inplace=True)

# Count of each class in the target column
data["TenYearCHD"].value_counts()

"""# Handle Missing Values, Select Features and Target Variables"""

msno.matrix(data)

# Handling Missing Values
data.dropna(inplace=True)

# Features (X) will now include systolic, diastolic, heart rate, age, and BMI
X = data[['age', 'BMI', 'sysBP', 'diaBP', 'heartRate', 'totChol', 'glucose']]  # Features
y = data['TenYearCHD']  # Target variable (Hypertension)
X.head()

y.head()

# Count of each class in the target column
y.value_counts()

# Handling Missing Values
msno.matrix(data)

numeric = ['cigsPerDay','totChol','BMI','heartRate','glucose']

data_clean = data.copy()
data_clean[numeric] = data_clean[numeric].fillna(data_clean[numeric].median())

data_clean['BPMeds'] = data['BPMeds'].fillna(data['BPMeds'].mode()[0])

data_clean.sample(10)

msno.matrix(data_clean)

"""## CHECKING INCONSISTENCIES"""

for col in data_clean.columns:
    print(f'{col} : {data[col].unique()}')

"""#Handling Outlier"""

numerical = data.select_dtypes(include=['int64', 'float64']).columns
len(numerical)

"""#Outlier detection can use the IQR Method or Z-score. Visually, it can be seen using a boxplot."""

numerical = data_clean.select_dtypes(include=['int64', 'float64']).columns

fig, axes = plt.subplots(5,3,figsize = (20,10))
fig.subplots_adjust(hspace=1, wspace=0.5)
row = 0
cols = 0

for item in numerical:
    if cols > 2:
        cols = 0
        row += 1
    ax = sns.boxplot(x=item,data=data_clean,ax=axes[row, cols])
    ax.set_ylabel("")
    cols += 1

"""Next, data cleaning from outliers is carried out using the Winsorizing method. This method functions to limit extreme values ​​(outliers) in numeric data so that they do not exceed a certain limit, by cutting values ​​that are outside the lower bound and upper bound to that limit, without deleting data. The implementation of Winsorizing on data is done by cutting outliers using the interquartile range (IQR) formula."""

def winsorizing_outlier(data,fitur):
  data_filtered = data.copy()
  for col in num_features:
    data = data_filtered[col]
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    data_filtered[col] = data_filtered[col].clip(lower_bound,upper_bound)

  return data_filtered

num_features = ['cigsPerDay','totChol','sysBP',
                'diaBP','BMI','heartRate','glucose']
df_tmp = winsorizing_outlier(data_clean,num_features)

numerical = df_tmp.select_dtypes(include=['int64', 'float64']).columns

fig, axes = plt.subplots(5,3,figsize = (20,10))
fig.subplots_adjust(hspace=1, wspace=0.5)
row = 0
cols = 0

for item in numerical:
    if cols > 2:
        cols = 0
        row += 1
    ax = sns.boxplot(x=item,data=df_tmp ,ax=axes[row, cols])
    ax.set_ylabel("")
    cols += 1

"""Correlation Matrix"""

Corr_Matrix = round(data[['age', 'BMI', 'sysBP', 'diaBP', 'heartRate', 'totChol', 'glucose']].corr(),2)
print(Corr_Matrix)

correlation_full_health = data[['age', 'BMI', 'sysBP', 'diaBP', 'heartRate', 'totChol', 'glucose']].corr()

axis_corr = sns.heatmap(correlation_full_health, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(50, 500, n=500), square=True)
plt.show()

"""# Standardize and split data

Handling Class Imabalance {Undersampling & Oversampling}
"""

# Specify the number of samples for both classes
# Get the number of samples in the minority class
minority_class_samples = y.value_counts()[1]  # Assuming 1 is the minority class

# Set up SMOTE for oversampling the minority class (class 1) to the original number of samples
# or a higher number if desired. Here, we're setting it to the original number.
smote = SMOTE(sampling_strategy={1: minority_class_samples}, random_state=42)

# Set up random undersampling to reduce the majority class (class 0) to the same number
# as the minority class after oversampling (minority_class_samples)
undersample = RandomUnderSampler(sampling_strategy={0: minority_class_samples}, random_state=42)

# Combine both oversampling and undersampling into a pipeline
pipeline = Pipeline(steps=[('o', smote), ('u', undersample)])

# Apply the sampling to the training data
X_resampled, y_resampled = pipeline.fit_resample(X, y)

# Check the distribution of the classes after resampling
print(f"Class distribution after resampling: {y_resampled.value_counts()}")

# Standardize the resampled data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Convert scaled data back to a DataFrame for easier visualization
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Visualize the distribution of scaled features
fig, axes = plt.subplots(4, 2, figsize=(15, 20))
fig.subplots_adjust(hspace=0.5, wspace=0.3)
axes = axes.ravel()

for i, col in enumerate(X_scaled_df.columns):
    sns.histplot(X_scaled_df[col], kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of Scaled {col}')
    axes[i].set_xlabel(f'Scaled {col}')
    axes[i].set_ylabel('Frequency')

# Hide the last unused subplot if the number of features is odd
if len(X_scaled_df.columns) % 2 != 0:
    fig.delaxes(axes[-1])

plt.suptitle('Distribution of Scaled Input Features', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 1.01])
plt.show()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Check the class distribution in y_train before resampling
print(f"Original class distribution in y_train: {y_train.value_counts()}")

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(f"X_train maxVal, {X_train.max()}, X_train minVal, {X_train.min()}")

# Count of each class in the target column
y_train.value_counts()

from sklearn.preprocessing import PowerTransformer

# Initialize the PowerTransformer with Yeo-Johnson method
yeo_johnson_transformer = PowerTransformer(method='yeo-johnson')

# Apply the transformation to the training data
X_train_transformed = yeo_johnson_transformer.fit_transform(X_train)

# Apply the transformation to the test data (using the transformer fitted on the training data)
X_test_transformed = yeo_johnson_transformer.transform(X_test)

# Convert the transformed data back to DataFrames for easier visualization and analysis
# We'll use the original column names
X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=X.columns)
X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=X.columns)


# Visualize the distribution of the transformed features
X_train_transformed_df.hist(bins=20, figsize=(15, 10))
plt.suptitle("Distribution of Features after Yeo-Johnson Transformation (Training Data)", y=1.02)
plt.tight_layout()
plt.show()

# You can also visualize the test data if needed
X_test_transformed_df.hist(bins=20, figsize=(15, 10))
plt.suptitle("Distribution of Features after Yeo-Johnson Transformation (Test Data)", y=1.02)
plt.tight_layout()
plt.show()

from sklearn.preprocessing import RobustScaler
# Initialize the RobustScaler
robust_scaler = RobustScaler()

# Apply the transformation to the training data
X_train_robust_scaled = robust_scaler.fit_transform(X_train_transformed)

# Apply the transformation to the test data (using the scaler fitted on the training data)
X_test_robust_scaled = robust_scaler.transform(X_test_transformed)

# Convert the robust scaled data back to DataFrames for easier visualization and analysis
X_train_robust_scaled_df = pd.DataFrame(X_train_robust_scaled, columns=X.columns)
X_test_robust_scaled_df = pd.DataFrame(X_test_robust_scaled, columns=X.columns)

# Visualize the distribution of the robust scaled features for training data
X_train_robust_scaled_df.hist(bins=20, figsize=(15, 10))
plt.suptitle("Distribution of Features after Robust Scaling (Training Data)", y=1.02)
plt.tight_layout()
plt.show()

# Visualize the distribution of the robust scaled features for test data
X_test_robust_scaled_df.hist(bins=20, figsize=(15, 10))
plt.suptitle("Distribution of Features after Robust Scaling (Test Data)", y=1.02)
plt.tight_layout()
plt.show()

# You can now use X_train_robust_scaled and X_test_robust_scaled for model training

"""# **Model Setup & Algorithms**

##Random Forest
"""

# Initialize the RandomForestClassifier
model = RandomForestClassifier(random_state=42, class_weight='balanced')

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train,y_train)

from sklearn.metrics import classification_report

y_pred = rf.predict(X_test)
print(classification_report(y_pred, y_test, digits = 5))

"""## Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train,y_train)

from sklearn.metrics import classification_report

y_pred = lr.predict(X_test)
print(classification_report(y_test,y_pred, digits = 5))

"""##XGBoost"""

from xgboost import XGBClassifier

xgb = XGBClassifier(random_state = 42)
xgb.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = xgb.predict(X_test)
print(classification_report(y_test, y_pred, digits = 5))

"""##KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train,y_train)

from sklearn.metrics import classification_report

y_pred = knn.predict(X_test)
print(classification_report(y_test,y_pred, digits = 5))

"""# Perform Grid Search to Select Best Model Hyperparameters for Training  """

# Define the hyperparameters for RandomizedSearchCV
param_dist = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 4, 5, 6, 7, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2'],
    'bootstrap': [True, False]
}

# Perform RandomizedSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the model with the best parameters found
random_search.fit(X_train, y_train)

# Get the best model from RandomizedSearchCV
best_model = random_search.best_estimator_

# Predict on the test set
y_pred = best_model.predict(X_test)

"""# **Evaluate Model and plot Confusion Matrix**"""

from sklearn.metrics import confusion_matrix

models = {
    'Logistic Regression': LogisticRegression(),
    'KNN': KNeighborsClassifier(),
    'Random Forest': RandomForestClassifier(),
    'XGBoost': XGBClassifier(random_state=42)
}

# Create subplots to display the heatmap.
fig, axes = plt.subplots(2, 2, figsize=(8, 6))
axes = axes.flatten()

for ax, (model_name, model) in zip(axes, models.items()):
    # Train model
    model.fit(X_train, y_train)

    # Prediction
    y_pred = model.predict(X_test)

    # confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # heatmap
    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')
    ax.set_title(model_name)
    ax.set_xlabel('Prediction')
    ax.set_ylabel('Actual')

# Showing plot
plt.tight_layout()
plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming y_test and y_pred are defined

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

"""# Save Model"""

import joblib
import pickle

# Save the best model and the scaler
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

print("Model and scaler saved successfully.")

# Save the trained model and scaler to files using joblib
model_filename = 'hypertension_model_rf_best_tuned.joblib'
scaler_filename = 'scaler_rf_best_tuned.joblib'

joblib.dump(best_model, model_filename)  # Save the best model
joblib.dump(scaler, scaler_filename)  # Save the scaler

print(f"Best model and scaler have been saved as {model_filename} and {scaler_filename}.")

"""# User Inputs"""

# Function to allow user input for prediction
def get_user_input_and_predict():
    print("Please enter the following information:")

    # User inputs
    age = float(input("age: "))  # Input for age
    bmi = float(input("BMI: "))  # Input for BMI
    systolic = float(input("sysBP: "))  # Input for systolic
    diastolic = float(input("diaBP: "))  # Input for diastolic
    heart_rate = float(input("heartRate: "))  # Input for heart rate
    cholestrol_level = float(input("totChol: "))  # Input for Cholestrol Levels
    glucose_level = float(input("glucose: "))  # Input for Glucose Levels

    # Create a DataFrame from the user input
    user_data = pd.DataFrame({
        'age': [age],
        'BMI': [bmi],
        'sysBP': [systolic],
        'diaBP': [diastolic],
        'heartRate': [heart_rate],
        'totChol': [cholestrol_level],
        'glucose': [glucose_level]
    })

    # Standardize the input data (important because the model was trained on scaled data)
    user_data_scaled = scaler.transform(user_data)

    # Set the threshold probability (e.g., 0.7)
    threshold = 0.5

    # Predict the hypertension probability using the trained model
    prediction_proba = best_model.predict_proba(user_data_scaled)[:, 1]  # Get the probability of hypertension
    prediction = (prediction_proba > threshold).astype(int)  # Apply the threshold

    # Display the predicted result
    print(f"Predicted Hypertension Probability: {prediction_proba[0]:.2f}")
    print(f"Predicted Hypertension: {'Yes' if prediction[0] == 1 else 'No'}")

    # Make recommendation based on probability
    if prediction_proba > threshold:
      print('Your predicted probability for heart disease is high. It is advised that you see a doctor for further evaluation.')
    else:
      print('Your predicted probability for heart disease is low. Keep up with a healthy lifestyle.')

    # 48 138 71 33.11 60

# Execute the function to get user input and predict
get_user_input_and_predict()

# AI (Explainable AI - Feature Importance)
# The best model from RandomizedSearchCV is 'best_model'

# Check if best_model is a tree-based model that supports feature_importances_
if hasattr(best_model, 'feature_importances_'):
    # Get feature importances from the best model
    feature_importances = best_model.feature_importances_
    features = X.columns

    # Create a DataFrame for better visualization
    feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

    # Sort features by importance
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    # Visualize feature importances
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
    plt.title('Feature Importance from Best Model')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()

    print("\nExplanation based on Feature Importance:")
    print("The plot above shows the relative importance of each feature in predicting hypertension risk.")
    print("Features with higher importance values contribute more significantly to the model's decision-making process.")
    print("Understanding these importances can help identify which factors are most predictive of hypertension.")
    print("For example, if 'sysBP' has the highest importance, it means systolic blood pressure is a crucial factor the model uses for prediction.")

elif hasattr(best_model, 'coef_'):
    # For linear models like Logistic Regression, use coefficients as a proxy for importance
    # Need to handle scaling effects if interpreting raw coefficients
    print("\nExplanation based on Model Coefficients (for Linear Models):")
    print("For linear models like Logistic Regression, coefficients indicate the direction and magnitude of the relationship")
    print("between each feature and the log-odds of the target variable.")
    print("A positive coefficient means an increase in the feature increases the likelihood of the positive class (hypertension).")
    print("A negative coefficient means an increase in the feature decreases the likelihood.")
    print("The magnitude of the coefficient indicates the strength of the relationship.")

    # Note: Interpreting raw coefficients after standardization can be complex,
    # but their relative magnitudes can still give a sense of importance.
    coefficients = best_model.coef_[0]
    features = X.columns
    coef_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})
    coef_df = coef_df.sort_values(by='Coefficient', key=abs, ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Coefficient', y='Feature', data=coef_df)
    plt.title('Feature Coefficients from Best Model')
    plt.xlabel('Coefficient')
    plt.ylabel('Feature')
    plt.show()


else:
    print("\nExplainable AI: Feature importance not directly available for the selected model type.")
    print("Consider using libraries like SHAP or LIME for model-agnostic explanations.")

# # prompt: add an explainable AI to the predicted output

# !pip install shap lime
# import shap
# import lime
# import lime.lime_tabular
# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np # Import numpy

# # AI (Explainable AI) - Using SHAP or LIME for model-agnostic explanations
# # Use SHAP or LIME to explain individual predictions

# # Choose a library based on preference and model type. SHAP is often preferred for its theoretical grounding.
# # Let's demonstrate with SHAP

# print("\n--- Explainable AI (Individual Prediction Explanation) ---")
# # Select a sample instance from the test set to explain its prediction
# # Let's choose the first instance from the test set for demonstration
# instance_to_explain = X_test_robust_scaled[0].reshape(1, -1) # Reshape for prediction
# # Convert the instance to a DataFrame for force_plot to handle feature names correctly
# instance_to_explain_df = pd.DataFrame(instance_to_explain, columns=X.columns)


# # If the best_model is tree-based (like RandomForestClassifier or XGBClassifier)
# if isinstance(best_model, (RandomForestClassifier, XGBClassifier)):
#     # SHAP Explainer for tree-based models
#     explainer = shap.TreeExplainer(best_model)
#     shap_values = explainer.shap_values(instance_to_explain)

#     # SHAP values for the predicted class (assuming class 1 is the positive class - Hypertension)
#     # If predicting probabilities, shap_values might be a list of arrays.
#     # For binary classification with predict_proba, it often returns SHAP values for class 0 and class 1.
#     # We typically focus on the SHAP values for the positive class (index 1).
#     if isinstance(shap_values, list):
#         shap_values_for_positive_class = shap_values[1]
#     else: # If it's a single array (e.g., for log-odds)
#         shap_values_for_positive_class = shap_values

#     print("\nSHAP explanation for the first test instance:")
#     # Visualize the explanation
#     shap.initjs() # Initialize JS visualization in notebooks
#     # Corrected order of parameters and reshape shap_values for force_plot
#     shap.force_plot(explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,
#                     shap_values_for_positive_class.reshape(1, -1), # Reshape SHAP values to (1, 7)
#                     instance_to_explain_df)


#     # A simpler text-based output or summary plot
#     # The shap_values_for_positive_class will be an array of SHAP values, one for each feature
#     feature_shap_values = dict(zip(X.columns, shap_values_for_positive_class[0]))
#     print("Feature SHAP values for the prediction:")
#     for feature, shap_value in feature_shap_values.items():
#         # Ensure shap_value is a single number before formatting
#         # Check if shap_value is a numpy array and get the scalar value if it is
#         if isinstance(shap_value, np.ndarray):
#             # If it's a numpy array, iterate through its elements
#             for val in shap_value:
#                 print(f"  {feature}: {val:.4f}")
#         else:
#             # Otherwise, format the single value
#             print(f"  {feature}: {shap_value:.4f}")


#     # You can also use shap.summary_plot for a global explanation across the dataset
#     # This might take some time depending on the size of your data
#     # print("\nGenerating SHAP summary plot (explaining model behavior overall)...")
#     # shap_values_test = explainer.shap_values(X_test_robust_scaled)
#     # if isinstance(shap_values_test, list):
#     #     shap_values_test_positive = shap_values_test[1]
#     # else:
#     #     shap_values_test_positive = shap_values_test
#     # shap.summary_plot(shap_values_test_positive, X_test_robust_scaled, feature_names=X.columns)


# # If the best_model is a linear model (like Logistic Regression)
# elif isinstance(best_model, LogisticRegression):
#      # SHAP Explainer for linear models
#     explainer = shap.LinearExplainer(best_model, X_train_robust_scaled)
#     shap_values = explainer.shap_values(instance_to_explain)

#     print("\nSHAP explanation for the first test instance (for Linear Model):")
#     feature_shap_values = dict(zip(X.columns, shap_values[0]))
#     print("Feature SHAP values for the prediction:")
#     for feature, shap_value in feature_shap_values.items():
#         # Ensure shap_value is a single number before formatting
#         if isinstance(shap_value, np.ndarray):
#              # If it's a numpy array, iterate through its elements
#             for val in shap_value:
#                 print(f"  {feature}: {val:.4f}")
#         else:
#             # Otherwise, format the single value
#             print(f"  {feature}: {shap_value:.4f}")

#     # You can also visualize the explanation
#     shap.initjs()
#     # Corrected order of parameters and reshape shap_values for force_plot
#     shap.force_plot(explainer.expected_value, shap_values.reshape(1, -1), instance_to_explain_df)

#     # shap.summary_plot(shap_values, X_test_robust_scaled, feature_names=X.columns)


# # If the best_model is a non-linear, non-tree model (like KNN or other complex models)
# else:
#     print("\nUsing LIME for model-agnostic explanation (for non-linear models):")
#     # LIME Explainer
#     # Need to provide the training data for the explainer to learn the data distribution
#     # and the prediction function of the model.
#     # We also need the feature names and class names.
#     class_names = ['No Hypertension', 'Hypertension'] # Define your class names

#     explainer = lime.lime_tabular.LimeTabularExplainer(training_data=X_train_robust_scaled,
#                                                       feature_names=X.columns,
#                                                       class_names=class_names,
#                                                       mode='classification')

#     # Explain the prediction for the instance
#     # The predict_proba method is required by LIME
#     explanation = explainer.explain_instance(data_row=instance_to_explain[0],
#                                              predict_fn=best_model.predict_proba,
#                                              num_features=len(X.columns)) # Number of features to include in explanation

#     print("\nLIME explanation for the first test instance:")
#     # Print the explanation as text
#     print(explanation.as_text())

#     # You can also visualize the explanation in a notebook
#     explanation.show_in_notebook(show_table=True, show_all=False)

# print("\n--- End of Explainable AI Section ---")

# # !pip install lime shap

# import lime
# import lime.lime_tabular
# import shap

# # Initialize the SHAP explainer
# # Use a background dataset for the explainer (e.g., a sample of the training data)
# # Ensure the background data is also scaled the same way as the training data
# explainer_shap = shap.Explainer(best_model, X_train_robust_scaled)

# # def get_user_input_and_predict():
# #     print("Please enter the following information:")

# #     # User inputs
# #     age = float(input("age: "))  # Input for age
# #     bmi = float(input("BMI: "))  # Input for BMI
# #     systolic = float(input("sysBP: "))  # Input for systolic
# #     diastolic = float(input("diaBP: "))  # Input for diastolic
# #     heart_rate = float(input("heartRate: "))  # Input for heart rate
# #     cholestrol_level = float(input("totChol: "))  # Input for Cholestrol Levels
# #     glucose_level = float(input("glucose: "))  # Input for Glucose Levels

# #     # Create a DataFrame from the user input
# #     user_data = pd.DataFrame({
# #         'age': [age],
# #         'BMI': [bmi],
# #         'sysBP': [systolic],
# #         'diaBP': [diastolic],
# #         'heartRate': [heart_rate],
# #         'totChol': [cholestrol_level],
# #         'glucose': [glucose_level]
# #     })

# #     # Standardize the input data (important because the model was trained on scaled data)
# #     # Apply the same transformation chain: PowerTransformer then RobustScaler
# #     user_data_transformed = yeo_johnson_transformer.transform(user_data)
# #     user_data_scaled = robust_scaler.transform(user_data_transformed)

# #     # Set the threshold probability (e.g., 0.7)
# #     threshold = 0.5

# #     # Predict the hypertension probability using the trained model
# #     prediction_proba = best_model.predict_proba(user_data_scaled)[:, 1]  # Get the probability of hypertension
# #     prediction = (prediction_proba > threshold).astype(int)  # Apply the threshold

# #     # Display the predicted result
# #     print(f"\nPredicted Hypertension Probability: {prediction_proba[0]:.2f}")
# #     print(f"Predicted Hypertension: {'Yes' if prediction[0] == 1 else 'No'}")

# #     # Make recommendation based on probability
# #     if prediction_proba > threshold:
# #         print('Your predicted probability for heart disease is high. It is advised that you see a doctor for further evaluation.')
# #     else:
# #         print('Your predicted probability for heart disease is low. Keep up with a healthy lifestyle.')

#     # Explainable AI (XAI)

#     # Using LIME
#     print("\nLIME Explanation:")
#     # Create a LIME explainer
#     # Need to provide the feature names and the training data for LIME
#     # LIME works on original feature values for interpretation, but we need the scaler and model that work on scaled data.
#     # We can create a prediction function that takes original data, scales it, and predicts.
#     def predict_fn(x):
#         # Ensure the input is a DataFrame with the correct columns before scaling
#         x_df = pd.DataFrame(x, columns=X.columns)
#         x_transformed = yeo_johnson_transformer.transform(x_df)
#         x_scaled = robust_scaler.transform(x_transformed)
#         return best_model.predict_proba(x_scaled)

#     # Use a sample of the original training data for LIME's training data parameter
#     # Make sure this sample is representative and not the resampled data that was scaled
#     # Need to go back to the original X_train (before any resampling or scaling)
#     # A better approach is to use a portion of the original X for LIME background.
#     # For simplicity here, we'll use the transformed and scaled training data but LIME's training_data parameter is usually the original data.
#     # A more correct way would involve training LIME on original X and having a prediction function that handles the scaling internally.
#     # Let's try using a sample of the scaled training data for explainer_lime's training_data.
#     explainer_lime = lime.lime_tabular.LimeTabularExplainer(
#         training_data=X_train_robust_scaled,
#         feature_names=X.columns.tolist(),
#         class_names=['No Hypertension', 'Hypertension'],
#         mode='classification'
#     )

#     # Explain the user's input
#     # The explain_instance function expects the instance in the same format as training_data (scaled)
#     # So we need to explain the scaled user data.
#     explanation_lime = explainer_lime.explain_instance(
#         data_row=user_data_scaled[0], # Explain the first (and only) row of scaled user data
#         predict_fn=best_model.predict_proba, # Use the model's predict_proba directly since we're providing scaled input
#         num_features=len(X.columns) # Explain all features
#     )

#     # Display the LIME explanation
#     explanation_lime.as_list()
#     explanation_lime.show_in_notebook(show_table=True, show_all=False)


#     # Using SHAP
#     print("\nSHAP Explanation:")
#     # Calculate SHAP values for the user's input
#     # explainer_shap expects input in the same format as the background data (scaled)
#     shap_values = explainer_shap(user_data_scaled)

#     # Visualize the SHAP explanation
#     shap.initjs()
#     # Select SHAP values for the positive class (Hypertension, index 1)
#     shap.plots.waterfall(shap_values[0][:, 1], show=True)

#     # You could also use other SHAP plots like force plot (less ideal for single instance in text mode)
#     # shap.plots.force(shap_values[0])

# # Execute the function to get user input, predict, and explain
# get_user_input_and_predict()

"""# **Best Model**

The selected model is the model with the smallest FN, followed by the best accuracy, recall, precision and f1-score.

The best model is **Random Forest** with:
1. FN value of 21 people
2. Accuracy of 89.74%
2. Recall of 90.75%
3. Precision of 89.25%
4. F-1 Score of 90.00%

The accuracy value is more than the model success criteria of 80%
"""